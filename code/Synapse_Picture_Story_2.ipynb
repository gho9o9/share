{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "元ネタ\nhttps://github.com/gho9o9/ADPE2E/blob/master/Lab/Lab3/Lab3.md",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# デモ「ソースデータの探索による新たなインサイトの獲得」\r\n",
        "\r\n",
        "# 動機：このレポートの元になっているデータの源流にある生データから新たなインサイトを得たい\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_0.png\"/>\r\n",
        "\r\n",
        "# 1. カタログ検索（Purview との統合により Synapse Studio 上からデータ資産の検索や資産詳細情報の参照が可能）\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_1.png\"/>\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_2.png\"/>\r\n",
        "\r\n",
        "# 2. アセット情報参照\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_3.png\"/>\r\n",
        "\r\n",
        "# 3. 系列（リネージ）確認\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_4.png\"/>\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_5.png\"/>\r\n",
        "\r\n",
        "# 4. 生データ確認\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_6.png\"/>\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_7.png\"/>\r\n",
        "\r\n",
        "# 5. 生データファイル確認\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_8.png\"/>\r\n",
        "\r\n",
        "# 6. 生データファイル探索\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_9.png\"/>\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_10.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 7. Spark による生データの探索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "df_taxidata = spark.read.load('abfss://sample@o9o9adlsgen2.dfs.core.windows.net/ADPE2E/raw/nyctaxidata-raw/yellow_tripdata_*-*.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df_taxidata.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7-1. Spark データフレームお作法による加工"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Use Data Frame API Operations to Filter Data\r\n",
        "display(df_taxidata.select(\"tpep_pickup_datetime\", \"passenger_count\", \"total_amount\").filter(\"passenger_count > 6 and total_amount > 50.0\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7-2 SQLによるデータ加工\r\n",
        "\r\n",
        "### 7-2-1. Spark一時テーブル化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Create Local Temp View\r\n",
        "df_taxidata.limit(10000).createOrReplaceTempView('NYCTaxiDataTable') # デモ用にデータ量絞り込み \r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 7-2-2. 使い慣れたSQLによる分析（コード頭のマジックコードでランタイムコンテキストと言語を切り替え可能）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--Use SQL to count NYC Taxi Data records\r\n",
        "select count(*) from NYCTaxiDataTable\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Use SQL to filter NYC Taxi Data records\r\n",
        "\r\n",
        "select cast(tpep_pickup_datetime as date) as pickup_date\r\n",
        "  , tpep_dropoff_datetime\r\n",
        "  , passenger_count\r\n",
        "  , total_amount\r\n",
        "from NYCTaxiDataTable\r\n",
        "where cast(tpep_pickup_datetime as date) = '2019-01-07'\r\n",
        "  and passenger_count > 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Use SQL to aggregate NYC Taxi Data records and visualize data\r\n",
        "select case payment_type\r\n",
        "            when 1 then 'Credit card'\r\n",
        "            when 2 then 'Cash'\r\n",
        "            when 3 then 'No charge'\r\n",
        "            when 4 then 'Dispute'\r\n",
        "            when 5 then 'Unknown'\r\n",
        "            when 6 then 'Voided trip'\r\n",
        "        end as PaymentType\r\n",
        "  , count(*) as TotalRideCount\r\n",
        "from NYCTaxiDataTable\r\n",
        "group by payment_type\r\n",
        "order by TotalRideCount desc\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7-3. リレーショナルデータベース上のデータの Spark データフレーム化 と 結合\r\n",
        "### 7-3-1. SQLDBテーブルにアクセスしSparkデータフレーム化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val df_location = spark.read.sqlanalytics(\"sqlpool.adpe2e.TaxiLocationLookup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 7-3-2. 該当データフレームを Spark 一時テーブル化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "df_location.createOrReplaceTempView(\"NYCTaxiLocation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 7-3-3. 複数のSparkテーブルを結合した複雑な分析クエリを実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select \r\n",
        "    VendorID\r\n",
        "    , cast(tpep_pickup_datetime as date) as PickUpDate\r\n",
        "    , concat(year(tpep_pickup_datetime), '-', format_string('%02d',month(tpep_pickup_datetime),'##')) as PickUpYearMonth --Partition Key\r\n",
        "    , cast(tpep_pickup_datetime as timestamp) as PickUpDateTime\r\n",
        "    , cast(tpep_dropoff_datetime as date) as DropOffDate\r\n",
        "    , cast(tpep_dropoff_datetime as timestamp) as DropOffDateTime\r\n",
        "    , passenger_count as PassengerCount\r\n",
        "    , trip_distance as TripDistance\r\n",
        "    , cast(PULocationID as int) as PickUpLocationID\r\n",
        "    , pu.Zone as PickUpLocationZone\r\n",
        "    , pu.Borough as PickUpLocationBorough\r\n",
        "    , cast(DOLocationID as int) as DropOffLocationID\r\n",
        "    , do.Zone as DropOffLocationZone\r\n",
        "    , do.Borough as DropOffLocationBorough\r\n",
        "    , cast(payment_type as int) as PaymentTypeID\r\n",
        "    , case payment_type\r\n",
        "            when 1 then 'Credit card'\r\n",
        "            when 2 then 'Cash'\r\n",
        "            when 3 then 'No charge'\r\n",
        "            when 4 then 'Dispute'\r\n",
        "            when 5 then 'Unknown'\r\n",
        "            when 6 then 'Voided trip'\r\n",
        "        end as PaymentTypeDescription\r\n",
        "    , cast(case when fare_amount < 0 then 0.00 else fare_amount end as decimal(8,2)) as FareAmount --Cleanse invalid data\r\n",
        "    , cast(case when extra < 0 then 0.00 else extra end as decimal(8,2)) as ExtraAmount --Cleanse invalid data\r\n",
        "    , cast(case when mta_tax < 0 then 0.00 else mta_tax end as decimal(8,2)) as MTATaxAmount --Cleanse invalid data\r\n",
        "    , cast(case when tip_amount < 0 then 0.00 else tip_amount end as decimal(8,2)) as TipAmount --Cleanse invalid data\r\n",
        "    , cast(case when tolls_amount < 0 then 0.00 else tolls_amount end as decimal(8,2)) as TollsAmount --Cleanse invalid data\r\n",
        "    , cast(case when improvement_surcharge < 0 then 0.00 else improvement_surcharge end as decimal(8,2)) as ImprovementSurchargeAmount --Cleanse invalid data\r\n",
        "    , cast(case when total_amount < 0 then 0.00 else total_amount end as decimal(8,2)) as TotalRideAmount --Cleanse invalid data\r\n",
        "from NYCTaxiDataTable as rides\r\n",
        "  join NYCTaxiLocation as pu\r\n",
        "    on rides.PULocationID = pu.LocationID\r\n",
        "  join NYCTaxiLocation as do\r\n",
        "    on rides.DOLocationID = do.LocationID\r\n",
        "where passenger_count > 0 --Data Cleanup Rules\r\n",
        "  and year(tpep_pickup_datetime) = 2019\r\n",
        "limit 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7-4. 分析クエリ結果を Spark テーブルとして永続化（Spark テーブル以外にも Dedicated SQL Pool や データレイク への永続化も容易）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "df_preped = spark.sql(\" \\\r\n",
        "    select \\\r\n",
        "        VendorID \\\r\n",
        "        , cast(tpep_pickup_datetime as date) as PickUpDate \\\r\n",
        "        , concat(year(tpep_pickup_datetime), '-', format_string('%02d',month(tpep_pickup_datetime),'##')) as PickUpYearMonth \\\r\n",
        "        , cast(tpep_pickup_datetime as timestamp) as PickUpDateTime \\\r\n",
        "        , cast(tpep_dropoff_datetime as date) as DropOffDate \\\r\n",
        "        , cast(tpep_dropoff_datetime as timestamp) as DropOffDateTime \\\r\n",
        "        , passenger_count as PassengerCount \\\r\n",
        "        , trip_distance as TripDistance \\\r\n",
        "        , cast(PULocationID as int) as PickUpLocationID \\\r\n",
        "        , pu.Zone as PickUpLocationZone \\\r\n",
        "        , pu.Borough as PickUpLocationBorough \\\r\n",
        "        , cast(DOLocationID as int) as DropOffLocationID \\\r\n",
        "        , do.Zone as DropOffLocationZone \\\r\n",
        "        , do.Borough as DropOffLocationBorough \\\r\n",
        "        , cast(payment_type as int) as PaymentTypeID \\\r\n",
        "        , case payment_type \\\r\n",
        "                when 1 then 'Credit card' \\\r\n",
        "                when 2 then 'Cash' \\\r\n",
        "                when 3 then 'No charge' \\\r\n",
        "                when 4 then 'Dispute' \\\r\n",
        "                when 5 then 'Unknown' \\\r\n",
        "                when 6 then 'Voided trip' \\\r\n",
        "            end as PaymentTypeDescription \\\r\n",
        "        , cast(case when fare_amount < 0 then 0.00 else fare_amount end as decimal(8,2)) as FareAmount \\\r\n",
        "        , cast(case when extra < 0 then 0.00 else extra end as decimal(8,2)) as ExtraAmount \\\r\n",
        "        , cast(case when mta_tax < 0 then 0.00 else mta_tax end as decimal(8,2)) as MTATaxAmount  \\\r\n",
        "        , cast(case when tip_amount < 0 then 0.00 else tip_amount end as decimal(8,2)) as TipAmount  \\\r\n",
        "        , cast(case when tolls_amount < 0 then 0.00 else tolls_amount end as decimal(8,2)) as TollsAmount  \\\r\n",
        "        , cast(case when improvement_surcharge < 0 then 0.00 else improvement_surcharge end as decimal(8,2)) as ImprovementSurchargeAmount  \\\r\n",
        "        , cast(case when total_amount < 0 then 0.00 else total_amount end as decimal(8,2)) as TotalRideAmount  \\\r\n",
        "    from NYCTaxiDataTable as rides \\\r\n",
        "    join NYCTaxiLocation as pu \\\r\n",
        "        on rides.PULocationID = pu.LocationID \\\r\n",
        "    join NYCTaxiLocation as do \\\r\n",
        "        on rides.DOLocationID = do.LocationID \\\r\n",
        "    where passenger_count > 0 --Data Cleanup Rules \\\r\n",
        "    and year(tpep_pickup_datetime) = 2019 \\\r\n",
        "\")\r\n",
        "df_preped.write.mode(\"overwrite\").saveAsTable(\"sparkdb01.adpe2e_NYCTaxiData_preped\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 8. Serverless SQL Pool クエリエンジン（アドホッククエリエンジン）による探索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8-1. 生データをロードしたデータフレームを Parquet ファイルとして保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_taxidata.limit(10000).write.parquet(\"abfss://sample@o9o9adlsgen2.dfs.core.windows.net/ADPE2E/raw/nyctaxidata-raw-parquet/\") # デモ用にデータ量絞り込み "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8-2. ファイルの探索１（ファイルパスを直接指定）\r\n",
        "\r\n",
        "### 8-2-1. Synapse Studio 上のエクスプローラから Parquet ファイルを出力したディレクトリを辿り 右クリック -> 上位100行を選択\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_11.png\"/>\r\n",
        "\r\n",
        "### 8-2-2. ファイルの種類で Parquet 形式を選択\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_12.png\"/>\r\n",
        "\r\n",
        "### 8-2-3. 生成されたサンプルスクリプトを実行\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_13.png\"/>\r\n",
        "\r\n",
        "### 8-2-4. サンプルスクリプトを編集し分析クエリを実行\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_14.png\"/>\r\n",
        "\r\n",
        "## 8-3. ファイルの探索２（外部テーブル化しファイルパスを隠蔽）\r\n",
        "\r\n",
        "## 8-3-1. Synapse Studio 上のエクスプローラから Parquet ファイルを出力したディレクトリを辿り 右クリック -> 外部テーブルの作成 \r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_15.png\"/>\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_16.png\"/>\r\n",
        "\r\n",
        "## 8-3-2. 外部テーブルの作成先のデータベースとテーブル名を指定 \r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_17.png\"/>\r\n",
        "\r\n",
        "## 8-3-3. 生成される外部テーブル化のためのスクリプトを実行\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_18.png\"/>\r\n",
        "\r\n",
        "## 8-3-4. 標準SQL互換クエリでデータを探索可能\r\n",
        "<img src=\"https://raw.githubusercontent.com/gho9o9/share/main/media/image/Demo_Lineage_19.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# データソースファイルの削除\r\n",
        "#   Ref https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python\r\n",
        "mssparkutils.fs \\\r\n",
        "    .rm('abfss://sample@o9o9adlsgen2.dfs.core.windows.net/ADPE2E/raw/nyctaxidata-raw-parquet/', True) "
      ]
    }
  ]
}
